{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bbc1088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported 1735 tickers to compustat_ticker_list.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CECO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PNW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PRG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker\n",
       "0    AIR\n",
       "1    AAL\n",
       "2   CECO\n",
       "3    PNW\n",
       "4    PRG"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your Compustat output\n",
    "comp = pd.read_csv(\"/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/compustat/fundamentals_compustat.csv\")\n",
    "\n",
    "# Clean and standardize tickers\n",
    "comp['tic'] = comp['tic'].astype(str).str.upper().str.strip()\n",
    "\n",
    "# Filter to tickers that look valid (letters/numbers only)\n",
    "comp = comp[comp['tic'].str.match(r'^[A-Z0-9\\.-]+$')]\n",
    "\n",
    "# Drop duplicates and save to CSV\n",
    "tickers = comp[['tic']].drop_duplicates().rename(columns={'tic': 'ticker'})\n",
    "tickers.to_csv(\"/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/compustat_ticker_list.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Exported {len(tickers)} tickers to compustat_ticker_list.csv\")\n",
    "tickers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cfbdb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[***************       32%                       ]  547 of 1735 completedFailed to get ticker 'GME' reason: Failed to perform, curl: (6) Could not resolve host: query2.finance.yahoo.com. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.\n",
      "[*********************100%***********************]  1735 of 1735 completed\n",
      "\n",
      "385 Failed downloads:\n",
      "['X', 'SKX', 'BRK.B', 'GMS', 'BHLB', 'OLO', 'FL']: YFPricesMissingError('possibly delisted; no price data found  (period=400d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "['SNOW', 'UDMY', 'IOVA', 'HSIC', 'ENTG', 'SVMB', 'PODD', 'CVS', 'MTDR', 'CFLT', 'MSFT', 'AFRM', 'KMT', 'SPSC', 'SKT', 'TROW', 'EIG', 'MPW', 'COLB', 'CTRA', 'PNC', 'CDP', 'ATEN', 'THG', 'YOU', 'TRNO', 'PAY', 'SHW', 'HLX', 'IIPR', 'CSWC', 'VRTX', 'MTCH', 'MTRN', 'RH', 'DAY', 'SITE', 'VICR', 'FIGS', 'SYK', 'CALM', 'ZS', 'PGRE', 'FNB', 'O', 'DXPE', 'LCID', 'OUT', 'MSGS', 'CHEF', 'WTFC', 'TRUP', 'YELP', 'MYRG', 'VYX', 'CMPO', 'DSGR', 'JBTM', 'HAS', 'BEAM', 'IPG', 'UBER', 'OVV', 'VMI', 'NUE', 'IDXX', 'CTRE', 'NFE', 'CVLT', 'SUN', 'CHE', 'EVCM', 'DT', 'CLH', 'CHCO', 'RPD', 'CNX', 'RBC', 'C', 'FRME', 'PINC', 'GEV', 'BKD', 'DCI', 'FHN', 'TPL', 'GXO', 'LLY', 'PCTY', 'EOSE', 'STAG', 'PK', 'CR', 'U', 'UMBF', 'GLPI', 'MHK', 'RSG', 'PSA', 'WH', 'D', 'SG', 'EXTR', 'SYBT', 'NEM', 'AWK', 'AMR', 'ASGN', 'TIC', 'DLX', 'REZI', 'KGS', 'MOD', 'TNL', 'GLP', 'NWL', 'RLJ', 'IRDM', 'MAR', 'MRTN', 'SNDX', 'PNFP', 'KO', 'IEX', 'FI', 'HOPE', 'MCY', 'KD', 'HIG', 'ANET', 'PDFS', 'FRHC', 'THS', 'NEO', 'SOFI', 'HOMB', 'SWX', 'WFC', 'AROC', 'KHC', 'KYMR', 'FSLY', 'AIZ', 'XIFR', 'MLI', 'BLK', 'OPCH', 'APLD', 'FDX', 'AMPH', 'KR', 'LINE', 'KEX', 'BF.B', 'SLVM', 'GIS', 'SRRK', 'ADSK', 'CECO', 'LSTR', 'CWST', 'BRKL', 'DINO', 'CVI', 'KVYO', 'MSCI', 'IRTC', 'MAT', 'ARLO', 'KEY', 'RBCAA', 'AYI', 'SDGR', 'ROKU', 'ROOT', 'CDNA', 'CPAY', 'STBA', 'OMF', 'MSI', 'UPWK', 'GH', 'JXN', 'XPO', 'MNST', 'OBK', 'ARCC', 'SAM', 'CSW', 'NTRS', 'VRRM', 'UCB', 'UP', 'DFIN', 'CTRI', 'BCPC', 'PYPL', 'CPK', 'DVA', 'ACHR', 'BELFB', 'REG', 'HD', 'GTM', 'PWP', 'DRVN', 'ARW', 'PI', 'CMC', 'NYT', 'EXAS', 'CDE', 'ON', 'BLD', 'MNR', 'XMTR', 'ADP', 'COP', 'CERT', 'DHR', 'FTV', 'SBRA', 'THO', 'PRK', 'KNX', 'AGNC', 'HRB', 'HWM', 'GL', 'FIVE', 'ETR', 'DJT', 'HUN', 'BKV', 'PRIM', 'APPN', 'EMN', 'EA', 'IDYA', 'AAP', 'MORN', 'MTZ', 'MODG', 'UDR', 'GPI', 'HLMN', 'UPS', 'FCPT', 'SABR', 'NSA', 'NATL', 'FOXA', 'TSLX', 'POWI', 'MLKN', 'LOAR', 'FLR', 'BXP', 'QLYS', 'AXP', 'ORLY', 'IP', 'SWK', 'DPZ', 'SAIC', 'KRP', 'PHR', 'TTD', 'WOR', 'DG', 'BTU', 'SIRI', 'DDS', 'AUB', 'VVX', 'CLX', 'VIR', 'GS', 'DUK', 'FSS', 'MD', 'SEPN', 'DOW', 'ROST', 'VIRT', 'NVST', 'GYRE', 'OC', 'NNI', 'EXC', 'BX', 'URI', 'VCTR', 'DKNG', 'OBDC', 'FTRE', 'NFLX', 'PRI', 'BPOP', 'KMB', 'AAL', 'TTGT', 'ITRI', 'TECH', 'KRG', 'MAC', 'WDC', 'BBAI', 'TNDM', 'NTCT', 'SEMR', 'WING', 'SMMT', 'NTLA', 'CFG', 'CSR', 'SILA', 'CLMT', 'SLM', 'MGY', 'PSX', 'BAH', 'ULS', 'MCW', 'MBIN', 'WSFS', 'OSCR', 'KLG', 'BROS', 'FIS', 'PSN', 'PEN', 'CNC', 'XPEL', 'RGTI', 'NABL', 'PLNT', 'OXM', 'NSSC', 'PCT', 'MTG', 'UAA', 'MXL', 'AMT', 'NIC', 'ENSG', 'HR', 'CRCT', 'CBRE', 'NPO', 'VRNT', 'FBRT', 'STRL', 'AKRO', 'PRM', 'WAY', 'COF', 'CQP', 'VNT', 'ALEX', 'FMCC', 'AHCO', 'AVB', 'TTAN', 'PEP', 'ZBRA', 'TEX', 'CPB', 'BUSE', 'EQH', 'SYNA', 'CBZ', 'VECO', 'LBRT', 'EL', 'PWR', 'CGON', 'DMLP']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 1350 1Y price returns (price-only, Yahoo comparable).\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# Load tickers\n",
    "tickers = pd.read_csv(\n",
    "    \"/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/compustat_ticker_list.csv\"\n",
    ")['ticker'].tolist()\n",
    "\n",
    "# Fetch 13 months to be safe\n",
    "data = yf.download(\n",
    "    tickers,\n",
    "    period=\"400d\",\n",
    "    interval=\"1d\",\n",
    "    auto_adjust=False,  # price-only, not dividend-adjusted\n",
    "    group_by='ticker',\n",
    "    threads=True\n",
    ")\n",
    "\n",
    "# Compute 1Y returns relative to exactly 1Y ago\n",
    "today = pd.Timestamp.today().normalize()\n",
    "year_ago = today - pd.Timedelta(days=365)\n",
    "\n",
    "rows = []\n",
    "for t in tickers:\n",
    "    try:\n",
    "        d = data[t][['Close']].dropna().reset_index()\n",
    "        d['ticker'] = t\n",
    "\n",
    "        # Align to nearest trading days\n",
    "        p_now = d.iloc[-1]['Close']\n",
    "        d_year_ago = d[d['Date'] >= year_ago].iloc[0]['Close']  # first after year_ago\n",
    "        ret_12m = (p_now / d_year_ago - 1) * 100\n",
    "\n",
    "        rows.append({'ticker': t, 'ret_12m_pct': ret_12m})\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "returns = pd.DataFrame(rows)\n",
    "returns.to_csv(\n",
    "    \"/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/ret_12m_priceonly.csv\",\n",
    "    index=False\n",
    ")\n",
    "print(f\"✅ Saved {len(returns)} 1Y price returns (price-only, Yahoo comparable).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6334ea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1735 tickers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1735 of 1735 completed\n",
      "\n",
      "11 Failed downloads:\n",
      "['X', 'SKX', 'BRK.B', 'GMS', 'BHLB', 'OLO', 'FL', 'BRKL', 'KLG']: YFPricesMissingError('possibly delisted; no price data found  (period=365d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "['OXY']: Timeout('Failed to perform, curl: (28) Operation timed out after 10002 milliseconds with 0 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n",
      "['BF.B']: YFPricesMissingError('possibly delisted; no price data found  (period=365d)')\n",
      "Processing prices: 100%|██████████| 1735/1735 [00:01<00:00, 1583.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 626,906 daily price observations to /Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/daily_prices_yfinance.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching fundamentals:   2%|▏         | 41/1735 [00:41<26:44,  1.06it/s]$BRK.B: possibly delisted; no timezone found\n",
      "Fetching fundamentals:   3%|▎         | 50/1735 [00:50<28:02,  1.00it/s]$BF.B: possibly delisted; no price data found  (1d 1927-01-10 -> 2025-12-16)\n",
      "Fetching fundamentals:  23%|██▎       | 402/1735 [08:46<21:26,  1.04it/s]  $FL: possibly delisted; no timezone found\n",
      "Fetching fundamentals:  38%|███▊      | 655/1735 [12:34<15:05,  1.19it/s]$GMS: possibly delisted; no timezone found\n",
      "Fetching fundamentals:  39%|███▉      | 684/1735 [12:58<12:28,  1.40it/s]$X: possibly delisted; no timezone found\n",
      "Fetching fundamentals:  64%|██████▍   | 1109/1735 [18:50<06:13,  1.68it/s]$OLO: possibly delisted; no timezone found\n",
      "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: OLO\"}}}\n",
      "Fetching fundamentals:  71%|███████▏  | 1238/1735 [20:25<06:39,  1.25it/s]$KLG: possibly delisted; no timezone found\n",
      "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: KLG\"}}}\n",
      "Fetching fundamentals:  80%|███████▉  | 1381/1735 [22:26<06:02,  1.02s/it]$BRKL: possibly delisted; no timezone found\n",
      "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: BRKL\"}}}\n",
      "Fetching fundamentals:  81%|████████▏ | 1413/1735 [22:55<04:44,  1.13it/s]$SKX: possibly delisted; no timezone found\n",
      "Fetching fundamentals:  83%|████████▎ | 1447/1735 [23:24<04:17,  1.12it/s]$BHLB: possibly delisted; no timezone found\n",
      "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: BHLB\"}}}\n",
      "Fetching fundamentals: 100%|██████████| 1735/1735 [27:18<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fundamentals for 1735 tickers to /Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/yf_fundamentals.csv\n",
      "  ticker  eps_trailing  div_cash_365d  div_yield     marketcap    price\n",
      "0    AIR          0.80          0.375       0.46  3.216431e+09   81.300\n",
      "1    AAL          0.87          0.500       3.13  1.053168e+10   15.955\n",
      "2   CECO          1.43          0.357       0.59  2.173747e+09   60.990\n",
      "3    PNW          4.85          4.490       5.12  1.049565e+10   87.690\n",
      "4    PRG          3.94          0.640       2.08  1.214841e+09   30.720\n",
      "5    ABT          7.97          2.910       2.29  2.212123e+11  127.100\n",
      "6    AMD          1.91            NaN        NaN  3.399921e+11  208.835\n",
      "7    ALK          1.21          1.775       3.38  6.092302e+09   52.525\n",
      "8   MATX         13.08          1.740       1.42  3.893519e+09  122.480\n",
      "9    ALX          7.14         18.000       8.25  1.114462e+09  218.210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === SETTINGS ===\n",
    "input_path = \"/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/compustat_ticker_list.csv\"\n",
    "output_prices = \"/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/daily_prices_yfinance.csv\"\n",
    "output_fundamentals = \"/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/yf_fundamentals.csv\"\n",
    "\n",
    "# === LOAD TICKERS ===\n",
    "tickers = pd.read_csv(input_path)['ticker'].dropna().unique().tolist()\n",
    "print(f\"Loaded {len(tickers)} tickers\")\n",
    "\n",
    "# === 1) Fetch DAILY adjusted prices ===\n",
    "data = yf.download(\n",
    "    tickers,\n",
    "    period=\"365d\",\n",
    "    interval=\"1d\",\n",
    "    auto_adjust=True,\n",
    "    group_by='ticker',\n",
    "    threads=True,\n",
    "    progress=True\n",
    ")\n",
    "\n",
    "rows = []\n",
    "for t in tqdm(tickers, desc=\"Processing prices\"):\n",
    "    try:\n",
    "        d = data[t][['Close']].dropna().reset_index()\n",
    "        d['ticker'] = t\n",
    "        rows.append(d)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "daily_prices = pd.concat(rows)\n",
    "daily_prices.rename(columns={'Date': 'date', 'Close': 'adj_close'}, inplace=True)\n",
    "daily_prices.to_csv(output_prices, index=False)\n",
    "print(f\"✅ Saved {len(daily_prices):,} daily price observations to {output_prices}\")\n",
    "\n",
    "# === 2) Fetch FUNDAMENTALS (EPS + Dividends) ===\n",
    "fundamentals = []\n",
    "for t in tqdm(tickers, desc=\"Fetching fundamentals\"):\n",
    "    try:\n",
    "        tk = yf.Ticker(t)\n",
    "\n",
    "        # Dividend data (cash per share)\n",
    "        div = tk.dividends\n",
    "        div_last_yr = div[div.index >= (div.index.max() - pd.Timedelta(days=365))] if len(div) > 0 else pd.Series([])\n",
    "        div_cash_365d = div_last_yr.sum() if not div_last_yr.empty else np.nan\n",
    "\n",
    "        # Market info (includes trailing EPS and current price)\n",
    "        info = tk.info\n",
    "        eps = info.get(\"trailingEps\", np.nan)\n",
    "        price = info.get(\"currentPrice\", np.nan)\n",
    "        marketcap = info.get(\"marketCap\", np.nan)\n",
    "\n",
    "        # Dividend yield: annual dividend / current price\n",
    "        div_yield = np.nan\n",
    "        if price and not np.isnan(price) and div_cash_365d and not np.isnan(div_cash_365d):\n",
    "            div_yield = div_cash_365d / price\n",
    "\n",
    "        fundamentals.append({\n",
    "            \"ticker\": t,\n",
    "            \"eps_trailing\": eps,\n",
    "            \"div_cash_365d\": div_cash_365d,\n",
    "            \"div_yield\": round(div_yield * 100, 2) if not np.isnan(div_yield) else np.nan,\n",
    "            \"marketcap\": marketcap,\n",
    "            \"price\": price\n",
    "        })\n",
    "    except Exception as e:\n",
    "        fundamentals.append({\n",
    "            \"ticker\": t,\n",
    "            \"eps_trailing\": np.nan,\n",
    "            \"div_cash_365d\": np.nan,\n",
    "            \"div_yield\": np.nan,\n",
    "            \"marketcap\": np.nan,\n",
    "            \"price\": np.nan\n",
    "        })\n",
    "\n",
    "fund_df = pd.DataFrame(fundamentals)\n",
    "fund_df.to_csv(output_fundamentals, index=False)\n",
    "print(f\"✅ Saved fundamentals for {len(fund_df)} tickers to {output_fundamentals}\")\n",
    "\n",
    "# === 3) Merge daily + fundamentals summary ===\n",
    "print(fund_df.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb8b0c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved merged master to: /Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/merged_master.csv\n",
      "Rows: 1,735\n",
      "ticker                      company  gvkey  gsector_code gsector_name  gind_code fund_datadate  marketcap_musd  marketcap_bn  book_equity       pb start_date   end_date   ret_12m  ret_12m_pct  price    marketcap  eps_trailing  div_cash_365d  div_yield\n",
      "  ARLP ALLIANCE RESOURCE PTNRS  -LP 122915            10       Energy     101020    2024-12-31       3366.7500          3.37     1832.747 1.836997 2024-12-16 2025-12-16  0.006648         0.66 23.580 3.028333e+09          1.89          3.300      13.99\n",
      "    AM        ANTERO MIDSTREAM CORP  31109            10       Energy     101020    2024-12-31       7234.4780          7.23     2115.171 3.420280 2024-12-16 2025-12-16  0.252005        25.20 17.690 8.463091e+09          0.98          1.125       6.36\n",
      "    AR        ANTERO RESOURCES CORP  18465            10       Energy     101020    2024-12-31      10906.3333         10.91     7021.650 1.553244 2024-12-16 2025-12-16  0.055731         5.57 33.440 1.033065e+10          1.89            NaN        NaN\n",
      "   APA                     APA CORP   1678            10       Energy     101020    2024-12-31       8437.0167          8.44     5280.000 1.597920 2024-12-16 2025-12-16  0.143053        14.31 23.815 8.520685e+09          4.18          1.250       5.25\n",
      "  AROC                 ARCHROCK INC  65009            10       Energy     101010    2024-12-31       4359.9315          4.36     1323.531 3.294167 2024-12-16 2025-12-16 -0.006076        -0.61 25.410 4.456143e+09          1.51          0.975       3.84\n",
      "  AESI   ATLAS ENERGY SOLUTIONS INC  41981            10       Energy     101010    2024-12-31       2444.6131          2.44     1036.556 2.358399 2024-12-16 2025-12-16 -0.560952       -56.10  9.595 1.595076e+09         -0.10          1.220      12.71\n",
      "   BKR              BAKER HUGHES CO  32106            10       Energy     101010    2024-12-31      40595.2789         40.60    16895.000 2.402798 2024-12-16 2025-12-16  0.095782         9.58 45.005 4.440976e+10          2.90          1.130       2.51\n",
      "   BKV                     BKV CORP  41799            10       Energy     101020    2024-12-31       2006.6991          2.01     1559.574 1.286697 2024-12-16 2025-12-16  0.119565        11.96 25.810 2.500217e+09          0.48            NaN        NaN\n",
      "   BSM      BLACK STONE MINERALS LP  23433            10       Energy     101020    2024-12-31       3076.1470          3.08     1129.439 2.723606 2024-12-16 2025-12-16  0.061110         6.11 13.630 2.887685e+09          1.16          1.725      12.66\n",
      "   WHD                   CACTUS INC  32938            10       Energy     101010    2024-12-31       3977.3507          3.98     1071.117 3.713274 2024-12-16 2025-12-16 -0.294866       -29.49 43.850 3.018640e+09          2.51          0.670       1.53\n",
      "↳ Saved Energy: 93 rows to /Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/merged_master_Energy.csv\n",
      "↳ Saved Materials: 71 rows to /Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/merged_master_Materials.csv\n",
      "↳ Saved Industrials: 269 rows to /Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/merged_master_Industrials.csv\n",
      "↳ Saved Consumer Discretionary: 206 rows to /Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/merged_master_ConsumerDiscretionary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/ps4jvmnj60vcm3c12vhqhcfm0000gn/T/ipykernel_10872/1475073095.py:115: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_calendar_12m_return)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/merged_master_ConsumerStaples'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 168\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sub.empty:\n\u001b[32m    167\u001b[39m     outp = BASE / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmerged_master_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname.replace(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     \u001b[43msub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m↳ Saved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sub)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/mech-coding/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/mech-coding/lib/python3.12/site-packages/pandas/core/generic.py:3989\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3978\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3980\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3981\u001b[39m     frame=df,\n\u001b[32m   3982\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3986\u001b[39m     decimal=decimal,\n\u001b[32m   3987\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3989\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3991\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3993\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3994\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4006\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/mech-coding/lib/python3.12/site-packages/pandas/io/formats/format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/mech-coding/lib/python3.12/site-packages/pandas/io/formats/csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/mech-coding/lib/python3.12/site-packages/pandas/io/common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/mech-coding/lib/python3.12/site-packages/pandas/io/common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: '/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/merged_master_ConsumerStaples'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --------- SETTINGS ---------\n",
    "BASE = Path(\"/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection\")\n",
    "\n",
    "# Inputs\n",
    "PATH_COMPUSTAT   = BASE / \"compustat/fundamentals_compustat.csv\"\n",
    "PATH_DAILY       = BASE / \"daily_prices_yfinance.csv\"\n",
    "PATH_YF_FUNDS    = BASE / \"yf_fundamentals.csv\"\n",
    "\n",
    "# Output\n",
    "PATH_MASTER      = BASE / \"merged_master.csv\"\n",
    "\n",
    "# Optional: save quick per-sector summaries\n",
    "SAVE_SECTOR_SPLITS = True\n",
    "# --------------------------------\n",
    "\n",
    "\n",
    "# 1) Load Compustat fundamentals and build P/B\n",
    "comp = pd.read_csv(PATH_COMPUSTAT)\n",
    "\n",
    "# Clean tickers\n",
    "comp[\"tic\"] = comp[\"tic\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "# Preferred stock: take first non-missing among pstkrv, pstkl, pstk\n",
    "pstk_series = comp[[\"pstkrv\", \"pstkl\", \"pstk\"]].copy()\n",
    "pstk_val = pstk_series.bfill(axis=1).iloc[:, 0].fillna(0)\n",
    "\n",
    "# Book equity and P/B (MKVALT is in millions USD)\n",
    "comp[\"book_equity\"] = comp[\"ceq\"].fillna(0) + pstk_val\n",
    "comp[\"pb\"] = comp[\"mkvalt\"] / comp[\"book_equity\"].replace(0, np.nan)\n",
    "\n",
    "# Optional pretties\n",
    "comp[\"pb\"] = comp[\"pb\"].astype(float)\n",
    "comp[\"marketcap_musd\"] = comp[\"mkvalt\"].astype(float)\n",
    "comp[\"marketcap_usd\"] = comp[\"marketcap_musd\"] * 1e6\n",
    "comp[\"marketcap_bn\"] = (comp[\"marketcap_usd\"] / 1e9).round(2)\n",
    "\n",
    "# Keep lean columns from Compustat\n",
    "keep_comp_cols = [\n",
    "    \"gvkey\", \"tic\", \"conm\", \"gsector\", \"gind\", \"datadate\",\n",
    "    \"marketcap_musd\", \"marketcap_bn\", \"pb\", \"book_equity\"\n",
    "]\n",
    "comp_small = comp[keep_comp_cols].rename(columns={\n",
    "    \"tic\": \"ticker\",\n",
    "    \"conm\": \"company\",\n",
    "    \"gsector\": \"gsector_code\",\n",
    "    \"gind\": \"gind_code\",\n",
    "    \"datadate\": \"fund_datadate\"\n",
    "})\n",
    "\n",
    "# Complete GICS Sector mapping (Compustat GSECTOR codes)\n",
    "GSECTOR_NAME = {\n",
    "    10: \"Energy\",\n",
    "    15: \"Materials\",\n",
    "    20: \"Industrials\",                  # ✅ your main focus\n",
    "    25: \"Consumer Discretionary\",\n",
    "    30: \"Consumer Staples / Defensive\", # ✅ your main focus\n",
    "    35: \"Health Care\",\n",
    "    40: \"Financials\",                   # ❌ exclude (P/B problematic)\n",
    "    45: \"Information Technology\",       # ✅ your main focus\n",
    "    50: \"Communication Services\",\n",
    "    55: \"Utilities\",                    # ❌ exclude (regulated assets)\n",
    "    60: \"Real Estate\"                   # ❌ exclude (book ≠ economic value)\n",
    "}\n",
    "comp_small[\"gsector_name\"] = comp_small[\"gsector_code\"].map(GSECTOR_NAME)\n",
    "\n",
    "\n",
    "# 2) Load daily prices, compute CALENDAR 12m total return (div-adjusted adj_close)\n",
    "prices = pd.read_csv(PATH_DAILY, parse_dates=[\"date\"])\n",
    "prices[\"ticker\"] = prices[\"ticker\"].astype(str).str.upper().str.strip()\n",
    "prices = prices.sort_values([\"ticker\", \"date\"])\n",
    "\n",
    "def compute_calendar_12m_return(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute 12-month return based on calendar time:\n",
    "    - end_date = last available trading day in df\n",
    "    - start_date = closest trading day on or before (end_date - 12 months)\n",
    "    \"\"\"\n",
    "    df = df.sort_values(\"date\")\n",
    "    end_row = df.iloc[-1]\n",
    "    end_date = end_row[\"date\"]\n",
    "    cutoff = end_date - pd.DateOffset(months=12)\n",
    "\n",
    "    start_df = df[df[\"date\"] <= cutoff]\n",
    "    if start_df.empty:\n",
    "        return pd.Series({\n",
    "            \"first_price\": np.nan,\n",
    "            \"last_price\": end_row[\"adj_close\"],\n",
    "            \"start_date\": pd.NaT,\n",
    "            \"end_date\": end_date,\n",
    "            \"ret_12m\": np.nan,\n",
    "            \"ret_12m_pct\": np.nan,\n",
    "        })\n",
    "\n",
    "    start_row = start_df.iloc[-1]\n",
    "\n",
    "    first_price = float(start_row[\"adj_close\"])\n",
    "    last_price = float(end_row[\"adj_close\"])\n",
    "    ret_12m = last_price / first_price - 1.0\n",
    "\n",
    "    return pd.Series({\n",
    "        \"first_price\": first_price,\n",
    "        \"last_price\": last_price,\n",
    "        \"start_date\": start_row[\"date\"],\n",
    "        \"end_date\": end_date,\n",
    "        \"ret_12m\": ret_12m,\n",
    "        \"ret_12m_pct\": round(ret_12m * 100, 2),\n",
    "    })\n",
    "\n",
    "agg = (\n",
    "    prices.groupby(\"ticker\", group_keys=False)\n",
    "          .apply(compute_calendar_12m_return)\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "# (Optional) quick sanity check: how far apart are start/end?\n",
    "# print((agg[\"end_date\"] - agg[\"start_date\"]).describe())\n",
    "\n",
    "\n",
    "# 3) Load Yahoo fundamentals (EPS, dividends, yield)\n",
    "yf = pd.read_csv(PATH_YF_FUNDS)\n",
    "yf[\"ticker\"] = yf[\"ticker\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "# Make sure expected columns exist (fill if missing)\n",
    "for col in [\"eps_trailing\", \"div_cash_365d\", \"div_yield\", \"marketcap\", \"price\"]:\n",
    "    if col not in yf.columns:\n",
    "        yf[col] = np.nan\n",
    "\n",
    "\n",
    "# 4) Merge: Compustat ← returns ← Yahoo fundamentals\n",
    "merged = (\n",
    "    comp_small\n",
    "    .merge(\n",
    "        agg[[\"ticker\", \"ret_12m\", \"ret_12m_pct\", \"start_date\", \"end_date\"]],\n",
    "        on=\"ticker\", how=\"left\"\n",
    "    )\n",
    "    .merge(\n",
    "        yf[[\"ticker\", \"eps_trailing\", \"div_cash_365d\", \"div_yield\", \"marketcap\", \"price\"]],\n",
    "        on=\"ticker\", how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5) Final tidy columns & sorting\n",
    "cols_order = [\n",
    "    \"ticker\", \"company\", \"gvkey\", \"gsector_code\", \"gsector_name\", \"gind_code\", \"fund_datadate\",\n",
    "    \"marketcap_musd\", \"marketcap_bn\", \"book_equity\", \"pb\",\n",
    "    \"start_date\", \"end_date\", \"ret_12m\", \"ret_12m_pct\",\n",
    "    \"price\", \"marketcap\", \"eps_trailing\", \"div_cash_365d\", \"div_yield\"\n",
    "]\n",
    "cols_order = [c for c in cols_order if c in merged.columns]\n",
    "merged = merged[cols_order].sort_values([\"gsector_code\", \"company\", \"ticker\"])\n",
    "\n",
    "# 6) Save master CSV\n",
    "merged.to_csv(PATH_MASTER, index=False)\n",
    "print(f\"✅ Saved merged master to: {PATH_MASTER}\")\n",
    "print(f\"Rows: {len(merged):,}\")\n",
    "print(merged.head(10).to_string(index=False))\n",
    "\n",
    "# 7) (Optional) Save per-sector splits for convenience\n",
    "if SAVE_SECTOR_SPLITS:\n",
    "    for code, name in GSECTOR_NAME.items():\n",
    "        sub = merged[merged[\"gsector_code\"] == code]\n",
    "        if not sub.empty:\n",
    "            outp = BASE / f\"merged_master_{name.replace(' ', '')}.csv\"\n",
    "            sub.to_csv(outp, index=False)\n",
    "            print(f\"↳ Saved {name}: {len(sub):,} rows to {outp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bea660e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,735 rows from merged_master.csv\n",
      "✅ Saved file with CLEAN positive P/B measures to: /Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/merged_master_with_pb.csv\n",
      "ticker                      company gsector_name  pb_fye  pb_current  pb_current_pctile valuation_label\n",
      "  ARLP ALLIANCE RESOURCE PTNRS  -LP       Energy    1.84        1.65           0.471264   Mid valuation\n",
      "    AM        ANTERO MIDSTREAM CORP       Energy    3.42        4.00           0.839080  High valuation\n",
      "    AR        ANTERO RESOURCES CORP       Energy    1.55        1.47           0.362069   Mid valuation\n",
      "   APA                     APA CORP       Energy    1.60        1.61           0.454023   Mid valuation\n",
      "  AROC                 ARCHROCK INC       Energy    3.29        3.37           0.804598  High valuation\n",
      "  AESI   ATLAS ENERGY SOLUTIONS INC       Energy    2.36        1.54           0.390805   Mid valuation\n",
      "   BKR              BAKER HUGHES CO       Energy    2.40        2.63           0.729885  High valuation\n",
      "   BKV                     BKV CORP       Energy    1.29        1.60           0.425287   Mid valuation\n",
      "   BSM      BLACK STONE MINERALS LP       Energy    2.72        2.56           0.701149  High valuation\n",
      "   WHD                   CACTUS INC       Energy    3.71        2.82           0.770115  High valuation\n",
      "ℹ️ 100 rows have invalid or nonpositive P/B (set to NaN).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========= PATHS =========\n",
    "PATH_MASTER = \"/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/merged_master.csv\"\n",
    "PATH_OUT    = \"/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/merged_master_with_pb.csv\"\n",
    "# =========================\n",
    "\n",
    "# 1️⃣ Load data\n",
    "merged = pd.read_csv(PATH_MASTER)\n",
    "print(f\"Loaded {len(merged):,} rows from merged_master.csv\")\n",
    "\n",
    "# 2️⃣ Clean variables\n",
    "merged[\"book_equity\"] = pd.to_numeric(merged[\"book_equity\"], errors=\"coerce\")\n",
    "merged[\"marketcap_musd\"] = pd.to_numeric(merged[\"marketcap_musd\"], errors=\"coerce\")\n",
    "merged[\"marketcap\"] = pd.to_numeric(merged[\"marketcap\"], errors=\"coerce\")\n",
    "\n",
    "# Replace zeros and negatives in book_equity with NaN\n",
    "merged.loc[merged[\"book_equity\"] <= 0, \"book_equity\"] = np.nan\n",
    "\n",
    "# 3️⃣ Compute P/B (fiscal-year-end and current)\n",
    "# marketcap_musd is in millions USD → divide by book_equity (same units)\n",
    "merged[\"pb_fye\"] = merged[\"marketcap_musd\"] / merged[\"book_equity\"]\n",
    "\n",
    "# marketcap from Yahoo is in USD → divide by (book_equity * 1e6)\n",
    "merged[\"pb_current\"] = merged[\"marketcap\"] / (merged[\"book_equity\"] * 1e6)\n",
    "\n",
    "# Drop or flag nonsensical (<= 0 or infinite) P/Bs\n",
    "merged.loc[~np.isfinite(merged[\"pb_current\"]), \"pb_current\"] = np.nan\n",
    "merged.loc[merged[\"pb_current\"] <= 0, \"pb_current\"] = np.nan\n",
    "\n",
    "merged.loc[~np.isfinite(merged[\"pb_fye\"]), \"pb_fye\"] = np.nan\n",
    "merged.loc[merged[\"pb_fye\"] <= 0, \"pb_fye\"] = np.nan\n",
    "\n",
    "# 4️⃣ Round for readability\n",
    "merged[\"pb_fye\"] = merged[\"pb_fye\"].round(2)\n",
    "merged[\"pb_current\"] = merged[\"pb_current\"].round(2)\n",
    "\n",
    "# 5️⃣ Compute sector-relative percentile of pb_current (only for valid values)\n",
    "merged[\"pb_current_pctile\"] = (\n",
    "    merged.groupby(\"gsector_code\")[\"pb_current\"]\n",
    "    .rank(pct=True, method=\"average\")\n",
    ")\n",
    "\n",
    "# 6️⃣ Create categorical valuation label (e.g. low/high)\n",
    "def label_val(p):\n",
    "    if pd.isna(p):\n",
    "        return np.nan\n",
    "    elif p <= 0.3:\n",
    "        return \"Low valuation\"\n",
    "    elif p >= 0.7:\n",
    "        return \"High valuation\"\n",
    "    else:\n",
    "        return \"Mid valuation\"\n",
    "\n",
    "merged[\"valuation_label\"] = merged[\"pb_current_pctile\"].apply(label_val)\n",
    "\n",
    "# 7️⃣ Save updated file\n",
    "merged.to_csv(PATH_OUT, index=False)\n",
    "print(f\"✅ Saved file with CLEAN positive P/B measures to: {PATH_OUT}\")\n",
    "\n",
    "# Quick preview\n",
    "cols = [\n",
    "    \"ticker\",\n",
    "    \"company\",\n",
    "    \"gsector_name\",\n",
    "    \"pb_fye\",\n",
    "    \"pb_current\",\n",
    "    \"pb_current_pctile\",\n",
    "    \"valuation_label\",\n",
    "]\n",
    "print(merged[cols].head(10).to_string(index=False))\n",
    "\n",
    "# Optional diagnostic\n",
    "n_invalid = merged[\"pb_current\"].isna().sum()\n",
    "print(f\"ℹ️ {n_invalid:,} rows have invalid or nonpositive P/B (set to NaN).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c11806c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated brackets based on price-only returns.\n",
      "Saved 450 LOW, 450 MED, 450 HIGH.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === 1. Load the merged dataset ===\n",
    "merged_path = \"/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/merged_master_with_pb.csv\"\n",
    "df = pd.read_csv(merged_path)\n",
    "\n",
    "# === 2. Merge in the corrected price-only returns ===\n",
    "returns_path = \"/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/ret_12m_priceonly.csv\"\n",
    "returns = pd.read_csv(returns_path)\n",
    "\n",
    "# Ensure proper merge key\n",
    "returns.rename(columns={\"ret_12m_pct\": \"ret_12m_priceonly_pct\"}, inplace=True)\n",
    "df = df.merge(returns, on=\"ticker\", how=\"left\")\n",
    "\n",
    "# === 3. Compute updated return brackets ===\n",
    "# Drop missing\n",
    "df = df.dropna(subset=[\"ret_12m_priceonly_pct\"])\n",
    "\n",
    "# Compute quantile cutoffs\n",
    "q1 = df[\"ret_12m_priceonly_pct\"].quantile(1/3)\n",
    "q2 = df[\"ret_12m_priceonly_pct\"].quantile(2/3)\n",
    "\n",
    "def classify_return(x):\n",
    "    if x <= q1:\n",
    "        return \"LOW\"\n",
    "    elif x <= q2:\n",
    "        return \"MED\"\n",
    "    else:\n",
    "        return \"HIGH\"\n",
    "\n",
    "df[\"return_band\"] = df[\"ret_12m_priceonly_pct\"].apply(classify_return)\n",
    "\n",
    "# === 4. Save updated subsets ===\n",
    "base_path = \"/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection\"\n",
    "\n",
    "df_low = df[df[\"return_band\"] == \"LOW\"]\n",
    "df_med = df[df[\"return_band\"] == \"MED\"]\n",
    "df_high = df[df[\"return_band\"] == \"HIGH\"]\n",
    "\n",
    "df_low.to_csv(f\"{base_path}/StockSelection_Low.csv\", index=False)\n",
    "df_med.to_csv(f\"{base_path}/StockSelection_Medium.csv\", index=False)\n",
    "df_high.to_csv(f\"{base_path}/StockSelection_High.csv\", index=False)\n",
    "\n",
    "# === 5. Save the fully updated master ===\n",
    "df.to_csv(f\"{base_path}/merged_master_updated.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Updated brackets based on price-only returns.\")\n",
    "print(f\"Saved {len(df_low)} LOW, {len(df_med)} MED, {len(df_high)} HIGH.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49cc3982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,735 rows from merged_master_with_pb.csv\n",
      "✅ Saved full subset (using updated master): candidate_subset_all.csv (rows=1735)\n",
      "✅ Saved prescreened subset (any band): candidate_subset_prescreened.csv (rows=0)\n",
      "✅ Saved band prescreen LOW: candidate_subset_LOW.csv (rows=0)\n",
      "↳ LOW: 0 rows\n",
      "✅ Saved band prescreen MED: candidate_subset_MED.csv (rows=0)\n",
      "↳ MED: 0 rows\n",
      "✅ Saved band prescreen HIGH: candidate_subset_HIGH.csv (rows=0)\n",
      "↳ HIGH: 0 rows\n",
      "✅ Saved LOW/TECH candidate pairs: candidate_pairs_LOW_TECH.csv (rows=0)\n",
      "✅ Saved MED/INDUSTRIALS candidate pairs: candidate_pairs_MED_INDUSTRIALS.csv (rows=0)\n",
      "✅ Saved HIGH/TECH candidate pairs: candidate_pairs_HIGH_TECH.csv (rows=0)\n",
      "✅ Saved FAMILIAR large-cap anchor pairs: candidate_pairs_ANCHORS_FAMILIAR.csv (rows=0)\n",
      "⚠️ No pairs for LOW/TECH. Consider widening return diff or loosening valuation split.\n",
      "⚠️ No pairs for MED/INDUSTRIALS. Consider widening return diff or loosening valuation split.\n",
      "⚠️ No pairs for HIGH/TECH. Consider widening return diff or loosening valuation split.\n",
      "⚠️ No pairs for FAMILIAR ANCHORS. Consider widening return diff or loosening valuation split.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= PATHS =========\n",
    "BASE = Path(\"/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection\")\n",
    "\n",
    "PATH_MASTER_IN   = BASE / \"merged_master_with_pb.csv\"    # <-- already has pb_current_pctile, valuation_label, return_band, etc.\n",
    "OUT_DIR = BASE / \"preselection\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prescreen outputs (kept for visibility)\n",
    "PATH_OUT_ALL   = OUT_DIR / \"candidate_subset_all.csv\"\n",
    "PATH_OUT_PRE   = OUT_DIR / \"candidate_subset_prescreened.csv\"\n",
    "PATH_OUT_LOW   = OUT_DIR / \"candidate_subset_LOW.csv\"\n",
    "PATH_OUT_MED   = OUT_DIR / \"candidate_subset_MED.csv\"\n",
    "PATH_OUT_HIGH  = OUT_DIR / \"candidate_subset_HIGH.csv\"\n",
    "\n",
    "# Candidate PAIRS outputs\n",
    "PATH_CAND_LOW_TECH        = OUT_DIR / \"candidate_pairs_LOW_TECH.csv\"\n",
    "PATH_CAND_MED_IND         = OUT_DIR / \"candidate_pairs_MED_INDUSTRIALS.csv\"\n",
    "PATH_CAND_HIGH_TECH       = OUT_DIR / \"candidate_pairs_HIGH_TECH.csv\"\n",
    "PATH_CAND_ANCHORS_FAMILIAR= OUT_DIR / \"candidate_pairs_ANCHORS_FAMILIAR.csv\"\n",
    "\n",
    "# ------- SETTINGS -------\n",
    "BANDS = {\n",
    "    \"LOW\":  (-25.0, -10.0),\n",
    "    \"MED\":  (  5.0,  20.0),\n",
    "    \"HIGH\": ( 40.0,  70.0),\n",
    "}\n",
    "SECTOR_TECH        = \"Information Technology\"\n",
    "SECTOR_INDUSTRIALS = \"Industrials\"\n",
    "\n",
    "TOP_N_PAIRS_PER_GROUP = 70         # number of pairs per core group\n",
    "TOP_N_ANCHOR_PAIRS    = 350        # number of familiar anchor pairs (set None to keep all)\n",
    "\n",
    "# Familiar anchors constraint\n",
    "ANCHOR_MIN_MKT_CAP = 10e10          # > $50bn\n",
    "EXCLUDE_SECTORS_FOR_ANCHORS = {\"Financials\", \"Real Estate\"}\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def save_csv(df: pd.DataFrame, path: Path, label: str = \"\"):\n",
    "    df.to_csv(path, index=False)\n",
    "    if label:\n",
    "        print(f\"✅ Saved {label}: {path.name} (rows={len(df)})\")\n",
    "\n",
    "def label_band(x_pct: float) -> str | None:\n",
    "    if pd.isna(x_pct):\n",
    "        return None\n",
    "    for name, (lo, hi) in BANDS.items():\n",
    "        if lo <= x_pct <= hi:\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "def generate_pair_candidates(df_band_scope: pd.DataFrame,\n",
    "                             ret_col=\"ret_12m_priceonly_pct\",\n",
    "                             val_pct_col=\"pb_current_pctile\",\n",
    "                             top_n=10,\n",
    "                             max_ret_diff_primary=3.0,\n",
    "                             max_ret_diff_fallback=5.0,\n",
    "                             low_thr=0.30,\n",
    "                             high_thr=0.70,\n",
    "                             # scoring weights\n",
    "                              w_val=3.0,                 # valuation contrast weight\n",
    "                             w_ret=2.0,                 # return closeness penalty weight\n",
    "                             # anchor-specific extras\n",
    "                             prefer_band: str | None = None,\n",
    "                             prefer_boost=(0.20, 0.07), # (both-in-band bonus, one-in-band bonus)\n",
    "                             marketcap_weight=0.0):     # >0 to reward large caps (anchors)\n",
    "    \"\"\"\n",
    "    Build MANY ranked pair candidates within df_band_scope:\n",
    "\n",
    "      1) strict valuation split (<=low_thr vs >=high_thr), |Δret| <= 3pp\n",
    "      2) strict + 5pp\n",
    "      3) loose halves (<0.5 vs >=0.5) + 3pp\n",
    "      4) loose halves + 5pp\n",
    "\n",
    "    Score = w_val*valuation_contrast - w_ret*(ret_diff/max_allowed)\n",
    "            + marketcap_weight * pair_mcap_pref\n",
    "            + band_bonus (if prefer_band is set)\n",
    "\n",
    "    pair_mcap_pref = average of each leg’s market-cap percentile within df_band_scope.\n",
    "    \"\"\"\n",
    "    req = {\"ticker\", ret_col, val_pct_col}\n",
    "    if not req.issubset(df_band_scope.columns):\n",
    "        return pd.DataFrame(columns=[])\n",
    "\n",
    "    df_ = df_band_scope.dropna(subset=[ret_col, val_pct_col, \"ticker\"]).copy()\n",
    "    if df_.empty:\n",
    "        return df_\n",
    "\n",
    "    # Precompute single-name market-cap percentile (ascending: larger cap => higher pct)\n",
    "    if \"marketcap\" in df_.columns:\n",
    "        mcap_rank = (\n",
    "            df_[[\"ticker\", \"marketcap\"]]\n",
    "            .assign(mcap_pct=lambda x: x[\"marketcap\"].rank(pct=True))\n",
    "        )\n",
    "    else:\n",
    "        mcap_rank = None\n",
    "\n",
    "    # Prepare strict / loose cohorts\n",
    "    lo_strict  = df_[df_[val_pct_col] <= low_thr].copy()\n",
    "    hi_strict  = df_[df_[val_pct_col] >= high_thr].copy()\n",
    "    lo_loose   = df_[df_[val_pct_col] <  0.5].copy()\n",
    "    hi_loose   = df_[df_[val_pct_col] >= 0.5].copy()\n",
    "\n",
    "    def _score(lo_df, hi_df, max_ret_diff):\n",
    "        if lo_df.empty or hi_df.empty:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        lo_df = lo_df.assign(__key=1)\n",
    "        hi_df = hi_df.assign(__key=1)\n",
    "        m = lo_df.merge(hi_df, on=\"__key\", suffixes=(\"_lo\",\"_hi\")).drop(columns=\"__key\")\n",
    "\n",
    "        # Filter self-pairs and enforce return closeness\n",
    "        m = m[m[\"ticker_lo\"] != m[\"ticker_hi\"]]\n",
    "        m[\"ret_diff_pp\"] = (m[f\"{ret_col}_lo\"] - m[f\"{ret_col}_hi\"]).abs()\n",
    "        m = m[m[\"ret_diff_pp\"] <= max_ret_diff]\n",
    "        if m.empty:\n",
    "            return m\n",
    "\n",
    "        # P/B percentile contrast\n",
    "        m[\"val_contrast\"] = (m[f\"{val_pct_col}_hi\"] - m[f\"{val_pct_col}_lo\"]).abs()\n",
    "\n",
    "        # Market-cap preference (only if requested)\n",
    "        if marketcap_weight > 0.0 and mcap_rank is not None:\n",
    "            m = m.merge(mcap_rank.rename(columns={\"ticker\":\"ticker_lo\", \"mcap_pct\":\"mcap_pct_lo\"}),\n",
    "                        on=\"ticker_lo\", how=\"left\")\n",
    "            m = m.merge(mcap_rank.rename(columns={\"ticker\":\"ticker_hi\", \"mcap_pct\":\"mcap_pct_hi\"}),\n",
    "                        on=\"ticker_hi\", how=\"left\")\n",
    "            m[\"pair_mcap_pref\"] = (m[\"mcap_pct_lo\"].fillna(0) + m[\"mcap_pct_hi\"].fillna(0)) / 2.0\n",
    "        else:\n",
    "            m[\"pair_mcap_pref\"] = 0.0\n",
    "\n",
    "        # Composite score\n",
    "        m[\"score\"] = (\n",
    "            w_val * m[\"val_contrast\"]\n",
    "            - w_ret * (m[\"ret_diff_pp\"] / max_ret_diff)\n",
    "            + marketcap_weight * m[\"pair_mcap_pref\"]\n",
    "        )\n",
    "\n",
    "        # Optional band preference (e.g., prefer MED for anchors)\n",
    "        if prefer_band is not None and \"return_band_lo\" in m.columns and \"return_band_hi\" in m.columns:\n",
    "            both = (m[\"return_band_lo\"] == prefer_band) & (m[\"return_band_hi\"] == prefer_band)\n",
    "            one  = ((m[\"return_band_lo\"] == prefer_band) ^ (m[\"return_band_hi\"] == prefer_band))\n",
    "            bonus = np.where(both, prefer_boost[0], np.where(one, prefer_boost[1], 0.0))\n",
    "            m[\"score\"] = m[\"score\"] + bonus\n",
    "\n",
    "        # Deduplicate unordered pairs, keep best scoring orientation\n",
    "        m[\"pair_key\"] = m.apply(lambda r: tuple(sorted([r[\"ticker_lo\"], r[\"ticker_hi\"]])), axis=1)\n",
    "        m = m.sort_values(\"score\", ascending=False).drop_duplicates(\"pair_key\", keep=\"first\")\n",
    "        return m\n",
    "\n",
    "    buckets = [\n",
    "        _score(lo_strict, hi_strict, max_ret_diff_primary),\n",
    "        _score(lo_strict, hi_strict, max_ret_diff_fallback),\n",
    "        _score(lo_loose,  hi_loose,  max_ret_diff_primary),\n",
    "        _score(lo_loose,  hi_loose,  max_ret_diff_fallback),\n",
    "    ]\n",
    "\n",
    "    out = pd.concat([b for b in buckets if b is not None and not b.empty], ignore_index=True)\n",
    "    if out.empty:\n",
    "        return out\n",
    "\n",
    "    out = out.sort_values(\"score\", ascending=False)\n",
    "\n",
    "    keep_cols = [\n",
    "        # left leg\n",
    "        \"ticker_lo\",\"company_lo\",\"gsector_name_lo\",\"return_band_lo\",\n",
    "        f\"{ret_col}_lo\", f\"{val_pct_col}_lo\",\n",
    "        \"marketcap_lo\",\"div_yield_lo\",\"eps_tra_lo\",\n",
    "        # right leg\n",
    "        \"ticker_hi\",\"company_hi\",\"gsector_name_hi\",\"return_band_hi\",\n",
    "        f\"{ret_col}_hi\", f\"{val_pct_col}_hi\",\n",
    "        \"marketcap_hi\",\"div_yield_hi\",\"eps_tra_hi\",\n",
    "        # diagnostics\n",
    "        \"ret_diff_pp\",\"val_contrast\",\"pair_mcap_pref\",\"score\"\n",
    "    ]\n",
    "    keep_cols = [c for c in keep_cols if c in out.columns]\n",
    "    return out[keep_cols].head(top_n) if (top_n is not None) else out[keep_cols]\n",
    "\n",
    "# ===================== 1) LOAD (NO RECOMPUTATION OF P/B) =====================\n",
    "df = pd.read_csv(PATH_MASTER_IN)\n",
    "print(f\"Loaded {len(df):,} rows from {PATH_MASTER_IN.name}\")\n",
    "\n",
    "# Ensure numerics for features we use\n",
    "for c in [\"ret_12m_pct\",\"ret_12m_priceonly_pct\",\"pb_current\",\"pb_current_pctile\",\"div_yield\",\"eps_trailing\",\"eps_tra\",\"marketcap\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "# Back-compat for EPS column\n",
    "if \"eps_trailing\" in df.columns and \"eps_tra\" not in df.columns:\n",
    "    df[\"eps_tra\"] = df[\"eps_trailing\"]\n",
    "\n",
    "# (Re)compute return bands from PRICE-ONLY returns to be safe\n",
    "if \"ret_12m_priceonly_pct\" in df.columns:\n",
    "    df[\"return_band\"] = df[\"ret_12m_priceonly_pct\"].apply(label_band)\n",
    "\n",
    "# Slim view for prescreen/debug\n",
    "keep_cols = [\n",
    "    \"ticker\",\"company\",\"gsector_name\",\"gsector_code\",\"return_band\",\n",
    "    \"ret_12m_pct\",\"ret_12m_priceonly_pct\",\n",
    "    \"eps_tra\",\"div_yield\",\n",
    "    \"pb_current\",\"pb_current_pctile\",\"valuation_label\",\n",
    "    \"marketcap\"\n",
    "]\n",
    "keep_cols = [c for c in keep_cols if c in df.columns]\n",
    "subset_all = df[keep_cols].copy()\n",
    "subset_all[\"marketcap_bn\"] = (subset_all[\"marketcap\"] / 1e9).round(2) if \"marketcap\" in subset_all else np.nan\n",
    "\n",
    "save_csv(subset_all, PATH_OUT_ALL, \"full subset (using updated master)\")\n",
    "\n",
    "# Prescreen by bands (using price-only)\n",
    "subset_all[\"return_band\"] = subset_all[\"ret_12m_priceonly_pct\"].apply(label_band) if \"ret_12m_priceonly_pct\" in subset_all else np.nan\n",
    "subset_pre = subset_all.dropna(subset=[\"return_band\"]).copy()\n",
    "save_csv(subset_pre, PATH_OUT_PRE, \"prescreened subset (any band)\")\n",
    "\n",
    "for band, path in [(\"LOW\", PATH_OUT_LOW), (\"MED\", PATH_OUT_MED), (\"HIGH\", PATH_OUT_HIGH)]:\n",
    "    sub = subset_pre[subset_pre[\"return_band\"] == band].copy()\n",
    "    save_csv(sub, path, f\"band prescreen {band}\")\n",
    "    print(f\"↳ {band}: {len(sub)} rows\")\n",
    "\n",
    "# ===================== 2) MANY PAIRS PER CORE GROUP (same-sector) =====================\n",
    "# LOW / Tech\n",
    "low_tech = subset_pre.query(\"return_band == 'LOW' and gsector_name == @SECTOR_TECH\").copy()\n",
    "cand_low_tech = generate_pair_candidates(\n",
    "    low_tech,\n",
    "    ret_col=\"ret_12m_priceonly_pct\",\n",
    "    val_pct_col=\"pb_current_pctile\",\n",
    "    top_n=TOP_N_PAIRS_PER_GROUP\n",
    ")\n",
    "save_csv(cand_low_tech, PATH_CAND_LOW_TECH, \"LOW/TECH candidate pairs\")\n",
    "\n",
    "# MED / Industrials\n",
    "med_ind = subset_pre.query(\"return_band == 'MED' and gsector_name == @SECTOR_INDUSTRIALS\").copy()\n",
    "cand_med_ind = generate_pair_candidates(\n",
    "    med_ind,\n",
    "    ret_col=\"ret_12m_priceonly_pct\",\n",
    "    val_pct_col=\"pb_current_pctile\",\n",
    "    top_n=TOP_N_PAIRS_PER_GROUP\n",
    ")\n",
    "save_csv(cand_med_ind, PATH_CAND_MED_IND, \"MED/INDUSTRIALS candidate pairs\")\n",
    "\n",
    "# HIGH / Tech\n",
    "high_tech = subset_pre.query(\"return_band == 'HIGH' and gsector_name == @SECTOR_TECH\").copy()\n",
    "cand_high_tech = generate_pair_candidates(\n",
    "    high_tech,\n",
    "    ret_col=\"ret_12m_priceonly_pct\",\n",
    "    val_pct_col=\"pb_current_pctile\",\n",
    "    top_n=TOP_N_PAIRS_PER_GROUP\n",
    ")\n",
    "save_csv(cand_high_tech, PATH_CAND_HIGH_TECH, \"HIGH/TECH candidate pairs\")\n",
    "\n",
    "# ===================== 3) FAMILIAR ANCHOR PAIRS (large-cap, cross-sector allowed, prefer MED) =====================\n",
    "anchors_pool = subset_all.copy()\n",
    "anchors_pool = anchors_pool[\n",
    "    (anchors_pool[\"marketcap\"].fillna(0) >= ANCHOR_MIN_MKT_CAP) &\n",
    "    (~anchors_pool[\"gsector_name\"].isin(EXCLUDE_SECTORS_FOR_ANCHORS))\n",
    "].copy()\n",
    "\n",
    "cand_anchors = generate_pair_candidates(\n",
    "    anchors_pool,\n",
    "    ret_col=\"ret_12m_priceonly_pct\",\n",
    "    val_pct_col=\"pb_current_pctile\",\n",
    "    top_n=TOP_N_ANCHOR_PAIRS,\n",
    "    prefer_band=\"MED\",            # prefer MED but don't require it\n",
    "    prefer_boost=(0.20, 0.07),\n",
    "    marketcap_weight=1.0          # reward larger caps in the pair\n",
    ")\n",
    "save_csv(cand_anchors, PATH_CAND_ANCHORS_FAMILIAR, \"FAMILIAR large-cap anchor pairs\")\n",
    "\n",
    "# ===================== 4) PREVIEW =====================\n",
    "def prev(df, name):\n",
    "    if df is None or df.empty:\n",
    "        print(f\"⚠️ No pairs for {name}. Consider widening return diff or loosening valuation split.\")\n",
    "    else:\n",
    "        cols = [c for c in [\"ticker_lo\",\"ticker_hi\",\"return_band_lo\",\"return_band_hi\",\"ret_diff_pp\",\"val_contrast\",\"pair_mcap_pref\",\"score\"] if c in df.columns]\n",
    "        print(f\"\\nTop pairs for {name}:\")\n",
    "        print(df[cols].head(10).to_string(index=False))\n",
    "\n",
    "prev(cand_low_tech,        \"LOW/TECH\")\n",
    "prev(cand_med_ind,         \"MED/INDUSTRIALS\")\n",
    "prev(cand_high_tech,       \"HIGH/TECH\")\n",
    "prev(cand_anchors,         \"FAMILIAR ANCHORS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d41d201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 20 very large-cap companies with 0.40 ≤ P/B ≤ 0.60:\n",
      "ticker                      company           gsector_name  pb_current  pb_current_pctile  ret_12m_priceonly_pct  marketcap_bn\n",
      "   BAC         BANK OF AMERICA CORP             Financials        1.31           0.416667              26.805354         387.5\n",
      "   WFC             WELLS FARGO & CO             Financials        1.53           0.513072              32.455330         273.6\n",
      "   COP               CONOCOPHILLIPS                 Energy        1.67           0.448276             -19.536244         108.4\n",
      "   XOM             EXXON MOBIL CORP                 Energy        1.86           0.528736              -1.789687         490.4\n",
      "    GS      GOLDMAN SACHS GROUP INC             Financials        2.00           0.614379              52.602408         244.6\n",
      "   CVX                 CHEVRON CORP                 Energy        2.04           0.574713               3.158175         310.5\n",
      "   PLD                 PROLOGIS INC            Real Estate        2.16           0.602564               9.996459         116.8\n",
      "   DHR                 DANAHER CORP            Health Care        3.09           0.397321             -12.460311         153.2\n",
      "   UNH       UNITEDHEALTH GROUP INC            Health Care        3.60           0.441964             -38.928255         333.1\n",
      "   CRM               SALESFORCE INC Information Technology        3.96           0.372093             -11.916121         242.1\n",
      "   RTX                     RTX CORP            Industrials        3.98           0.488372              46.640219         239.6\n",
      "  TMUS              T-MOBILE US INC Communication Services        3.99           0.629032              -5.605848         246.1\n",
      "   TMO THERMO FISHER SCIENTIFIC INC            Health Care        4.25           0.513393               1.812118         210.6\n",
      "    MU        MICRON TECHNOLOGY INC Information Technology        4.60           0.437209             124.796780         249.1\n",
      "   ABT          ABBOTT LABORATORIES            Health Care        4.62           0.558036               9.967359         220.4\n",
      "   MRK               MERCK & CO INC            Health Care        4.69           0.562500             -15.676311         217.4\n",
      "  CSCO            CISCO SYSTEMS INC Information Technology        6.13           0.520930              33.120327         287.1\n",
      "   AMD       ADVANCED MICRO DEVICES Information Technology        7.27           0.581395              76.886224         418.7\n",
      "   TXN        TEXAS INSTRUMENTS INC Information Technology        8.97           0.627907             -20.993310         151.7\n",
      "  INTU                   INTUIT INC Information Technology        9.60           0.646512               7.517615         189.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ========= PATH =========\n",
    "PATH_MASTER = \"/Users/paulgrass/Library/Mobile Documents/com~apple~CloudDocs/Documents/Model Spillovers/Data/Stock Selection/merged_master_updated.csv\"\n",
    "# ========================\n",
    "\n",
    "# 1️⃣ Load data\n",
    "df = pd.read_csv(PATH_MASTER)\n",
    "\n",
    "# 2️⃣ Ensure numeric consistency\n",
    "df[\"marketcap\"] = pd.to_numeric(df[\"marketcap\"], errors=\"coerce\")\n",
    "df[\"pb_current_pctile\"] = pd.to_numeric(df[\"pb_current_pctile\"], errors=\"coerce\")\n",
    "\n",
    "# 3️⃣ Define \"very large cap\" threshold (>= $100B)\n",
    "VERY_LARGE_CAP = 1e11  # USD\n",
    "\n",
    "# 4️⃣ Filter: large cap and mid-range valuation (0.40–0.60)\n",
    "mask = (\n",
    "    (df[\"marketcap\"] >= VERY_LARGE_CAP)\n",
    "    & (df[\"pb_current_pctile\"].between(0.33, 0.66))\n",
    ")\n",
    "\n",
    "large_midpb = df.loc[mask, [\n",
    "    \"ticker\",\n",
    "    \"company\",\n",
    "    \"gsector_name\",\n",
    "    \"pb_current\",\n",
    "    \"pb_current_pctile\",\n",
    "    \"marketcap\",\n",
    "    \"ret_12m_priceonly_pct\"\n",
    "]].sort_values(\"pb_current\")\n",
    "\n",
    "# 5️⃣ Format market cap in billions for readability\n",
    "large_midpb[\"marketcap_bn\"] = (large_midpb[\"marketcap\"] / 1e9).round(1)\n",
    "large_midpb = large_midpb.drop(columns=\"marketcap\")\n",
    "\n",
    "# 6️⃣ Print results\n",
    "print(f\"✅ Found {len(large_midpb)} very large-cap companies with 0.40 ≤ P/B ≤ 0.60:\")\n",
    "print(large_midpb.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe498c9f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mech-coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
